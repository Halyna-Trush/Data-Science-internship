{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyPSikDNEZIBLcs2Tkq+Lm/f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Halyna Trush.\n","Contact Information\n","Phone: +380954200758\n","Email: frolova.galka@gmail.com\n","LinkedIn: https://www.linkedin.com/in/halyna-trush/"],"metadata":{"id":"mGOAAiQrkHYK"}},{"cell_type":"markdown","source":["This notebook implements a complete pipeline for Sentinel-2 satellite image matching using both classical and deep-learning approaches.\n","It reads .jp2 images from Google Drive or a local folder, automatically detects keypoints, and compares image pairs captured in different seasons.\n","The workflow includes ORB + RANSAC (classical matching) and LoFTR (Kornia) (deep feature matching), computing metrics such as the number of matches, inlier ratio, reprojection error, and runtime.\n","The interactive interface allows users to select image pairs and rerun analysis without editing the code, ensuring full reproducibility and easy testing."],"metadata":{"id":"grkRQpTSkzT8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNpxozSMq04N"},"outputs":[],"source":["# Install dependencies (run once in Colab)\n","!pip install -q rasterio kornia\n","\n","import os\n","import cv2\n","import numpy as np\n","import torch\n","import kornia as K\n","import matplotlib.pyplot as plt\n","import rasterio\n","from rasterio.plot import reshape_as_image\n","!pip install rasterio\n","from google.colab import drive\n"]},{"cell_type":"code","source":["def read_image_any(path):\n","    \"\"\"\n","    Read an image from a given path.\n","\n","    - If extension is .jp2: read with rasterio (Sentinel-2 TCI, etc.).\n","    - Otherwise: read with OpenCV and convert BGR -> RGB.\n","\n","    Returns:\n","        img (np.ndarray): RGB image of shape (H, W, 3), dtype uint8.\n","    \"\"\"\n","    ext = os.path.splitext(path)[1].lower()\n","\n","    if ext == \".jp2\":\n","        # Read multi-band raster data in (C, H, W) format\n","        with rasterio.open(path, \"r\") as src:\n","            img = src.read()\n","\n","        # Convert to (H, W, C)\n","        img = reshape_as_image(img)\n","\n","        # Rescale to uint8 range [0, 255]\n","        img = img.astype(np.float32)\n","        max_val = img.max()\n","        if max_val > 0:\n","            img = img / max_val * 255.0\n","        img = img.clip(0, 255).astype(np.uint8)\n","\n","    else:\n","        # Fallback for non-JP2 formats: use OpenCV\n","        bgr = cv2.imread(path, cv2.IMREAD_COLOR)\n","        if bgr is None:\n","            raise ValueError(f\"Cannot read image: {path}\")\n","        img = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","\n","    return img\n"],"metadata":{"id":"hhBPb7gfq70y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_orb_matching(img1, img2, n_features=2000, ransac_thresh=5.0):\n","    \"\"\"\n","    Classical pipeline:\n","    - ORB keypoints + descriptors\n","    - brute-force matching\n","    - RANSAC homography to keep geometrically consistent matches\n","    - performance metrics and visualization\n","    \"\"\"\n","    import time\n","\n","    start_time = time.time()\n","\n","    # Convert to grayscale\n","    gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n","    gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n","\n","    # ORB detector\n","    orb = cv2.ORB_create(nfeatures=n_features)\n","\n","    kp1, des1 = orb.detectAndCompute(gray1, None)\n","    kp2, des2 = orb.detectAndCompute(gray2, None)\n","\n","    if des1 is None or des2 is None:\n","        print(\"No descriptors found in one of the images.\")\n","        return\n","\n","    # Brute-force matcher with Hamming distance\n","    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","    matches = bf.match(des1, des2)\n","    matches = sorted(matches, key=lambda m: m.distance)\n","\n","    raw_matches = len(matches)\n","    print(f\"Raw ORB matches: {raw_matches}\")\n","\n","    if raw_matches < 4:\n","        print(\"Not enough matches for RANSAC.\")\n","        return\n","\n","    # Extract coordinates\n","    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n","    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n","\n","    # RANSAC homography\n","    H, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC, ransacReprojThreshold=ransac_thresh)\n","    if H is None or mask is None:\n","        print(\"Homography could not be estimated.\")\n","        return\n","\n","    matches_mask = mask.ravel().tolist()\n","    inliers = int(np.sum(matches_mask))\n","    inlier_ratio = inliers / raw_matches if raw_matches > 0 else 0.0\n","\n","    # Reprojection error for inliers\n","    pts1_inliers = pts1[matches_mask == np.array(1)]\n","    pts2_inliers = pts2[matches_mask == np.array(1)]\n","    if len(pts1_inliers) > 0:\n","        pts1_proj = cv2.perspectiveTransform(pts1_inliers, H)\n","        errors = np.linalg.norm(pts2_inliers - pts1_proj, axis=2)\n","        mean_error = float(errors.mean())\n","        median_error = float(np.median(errors))\n","    else:\n","        mean_error = np.nan\n","        median_error = np.nan\n","\n","    end_time = time.time()\n","    runtime = end_time - start_time\n","\n","    print(f\"Inliers after RANSAC: {inliers} / {raw_matches} ({100 * inlier_ratio:.2f}%)\")\n","    print(f\"Mean reprojection error: {mean_error:.2f} px\")\n","    print(f\"Median reprojection error: {median_error:.2f} px\")\n","    print(f\"Runtime: {runtime:.2f} seconds\")\n","\n","    # Draw inlier matches only\n","    inlier_matches = [m for i, m in enumerate(matches) if matches_mask[i]]\n","\n","    matched_img = cv2.drawMatches(\n","        img1, kp1,\n","        img2, kp2,\n","        inlier_matches,\n","        None,\n","        matchColor=(0, 255, 0),\n","        singlePointColor=None,\n","        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n","    )\n","\n","    plt.figure(figsize=(14, 7))\n","    plt.imshow(matched_img)\n","    plt.title(\"ORB + RANSAC inlier matches\")\n","    plt.axis(\"off\")\n","    plt.show()\n"],"metadata":{"id":"x1NG7sDwsMJC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LoFTR_Matcher:\n","    def __init__(self, image_size=None, device=None):\n","        \"\"\"\n","        image_size: optional (H, W) to resize images before matching.\n","        device: 'cuda' or 'cpu'; if None, choose automatically.\n","        \"\"\"\n","        self.image_size = image_size\n","        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.matcher = K.feature.LoFTR(pretrained=\"outdoor\").eval().to(self.device)\n","\n","    def _convert_image(self, image):\n","        \"\"\"\n","        Convert H×W×3 uint8 RGB image to [1, 3, H, W] float tensor in [0, 1].\n","        Optionally resize if image is too large or image_size is specified.\n","        \"\"\"\n","        SIZE_MAX = 1280\n","        img = K.utils.image_to_tensor(image)  # [3, H, W]\n","        img = img.float().unsqueeze(0).to(self.device) / 255.0\n","\n","        if self.image_size is not None:\n","            img = K.geometry.resize(img, self.image_size, interpolation=\"area\")\n","        elif max(img.shape[-2], img.shape[-1]) > SIZE_MAX:\n","            img = K.geometry.resize(img, SIZE_MAX, side=\"long\", interpolation=\"area\")\n","\n","        return img\n","\n","    def __call__(self, image0, image1, confidence_min=0.8):\n","        \"\"\"\n","        Run LoFTR on two RGB uint8 images.\n","\n","        Returns:\n","            dict with:\n","                keypoints0: (N, 2) array of points in image0\n","                keypoints1: (N, 2) array of points in image1\n","                confidence: (N,) array of match confidences\n","                inliers: (N, 1) boolean array (MAGSAC inliers)\n","                fundamental: 3x3 fundamental matrix or None\n","        \"\"\"\n","        img0_t = self._convert_image(image0)\n","        img1_t = self._convert_image(image1)\n","\n","        inp = {\n","            \"image0\": K.color.rgb_to_grayscale(img0_t),\n","            \"image1\": K.color.rgb_to_grayscale(img1_t),\n","        }\n","\n","        with torch.inference_mode():\n","            corresp = self.matcher(inp)\n","\n","        # Confidence filter\n","        mask = corresp[\"confidence\"] > confidence_min\n","        idx = torch.nonzero(mask, as_tuple=True)\n","\n","        keypoints0 = corresp[\"keypoints0\"][idx].cpu().numpy()\n","        keypoints1 = corresp[\"keypoints1\"][idx].cpu().numpy()\n","        confidence = corresp[\"confidence\"][idx].cpu().numpy()\n","\n","        fundamental = None\n","        try:\n","            fmat, inliers = cv2.findFundamentalMat(\n","                keypoints0,\n","                keypoints1,\n","                cv2.USAC_MAGSAC,\n","                1.0,\n","                0.99,\n","                100000\n","            )\n","            if inliers is None:\n","                inliers = np.zeros((keypoints0.shape[0], 1), dtype=bool)\n","            else:\n","                inliers = inliers.astype(bool)\n","            fundamental = fmat\n","        except Exception:\n","            inliers = np.zeros((keypoints0.shape[0], 1), dtype=bool)\n","\n","        return {\n","            \"keypoints0\": keypoints0,\n","            \"keypoints1\": keypoints1,\n","            \"confidence\": confidence,\n","            \"inliers\": inliers,\n","            \"fundamental\": fundamental,\n","        }\n"],"metadata":{"id":"B-sNjXdBsQmM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_loftr_matching(img1, img2, confidence_min=0.8, max_draw=200):\n","    \"\"\"\n","    Run LoFTR on two images, print basic metrics and visualize inlier matches.\n","\n","    Metrics:\n","        - total matched keypoints (after confidence filter)\n","        - number of inliers (MAGSAC)\n","        - inlier ratio\n","        - runtime (seconds)\n","    \"\"\"\n","    import time\n","\n","    matcher = LoFTR_Matcher()\n","    img0_u8 = img1.astype(np.uint8)\n","    img1_u8 = img2.astype(np.uint8)\n","\n","    start_time = time.time()\n","    results = matcher(img0_u8, img1_u8, confidence_min=confidence_min)\n","    end_time = time.time()\n","    runtime = end_time - start_time\n","\n","    num_total = len(results[\"keypoints0\"])\n","    num_inliers = int(results[\"inliers\"].sum()) if results[\"inliers\"] is not None else 0\n","    ratio = num_inliers / num_total if num_total > 0 else 0.0\n","\n","    print(\"\\nLoFTR stats:\")\n","    print(f\"  Total matched keypoints: {num_total}\")\n","    print(f\"  Inliers (MAGSAC):       {num_inliers}\")\n","    print(f\"  Inlier ratio:           {ratio:.2f}\")\n","    print(f\"  Runtime:                {runtime:.2f} seconds\")\n","\n","    if num_inliers == 0:\n","        print(\"No inliers found, nothing to visualize.\")\n","        return\n","\n","    # Extract inlier coordinates\n","    inlier_mask = results[\"inliers\"].ravel().astype(bool)\n","    kp0 = results[\"keypoints0\"][inlier_mask]\n","    kp1 = results[\"keypoints1\"][inlier_mask]\n","\n","    # Random subsample for visualization\n","    if kp0.shape[0] > max_draw:\n","        sel_idx = np.random.choice(kp0.shape[0], max_draw, replace=False)\n","        kp0 = kp0[sel_idx]\n","        kp1 = kp1[sel_idx]\n","\n","    # Build side-by-side canvas\n","    h0, w0, _ = img0_u8.shape\n","    h1, w1, _ = img1_u8.shape\n","    canvas_h = max(h0, h1)\n","    canvas_w = w0 + w1\n","    canvas = np.zeros((canvas_h, canvas_w, 3), dtype=np.uint8)\n","    canvas[:h0, :w0] = cv2.cvtColor(img0_u8, cv2.COLOR_RGB2BGR)\n","    canvas[:h1, w0:w0 + w1] = cv2.cvtColor(img1_u8, cv2.COLOR_RGB2BGR)\n","\n","    # Draw matches in green\n","    for (x0, y0), (x1, y1) in zip(kp0, kp1):\n","        p1 = (int(x0), int(y0))\n","        p2 = (int(x1) + w0, int(y1))\n","        cv2.circle(canvas, p1, 3, (0, 255, 0), -1)\n","        cv2.circle(canvas, p2, 3, (0, 255, 0), -1)\n","        cv2.line(canvas, p1, p2, (0, 255, 0), 1)\n","\n","    plt.figure(figsize=(18, 9))\n","    plt.imshow(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n","    plt.axis(\"off\")\n","    plt.title(\"LoFTR inlier matches (subset)\")\n","    plt.show()\n"],"metadata":{"id":"7-fJdocnsgGY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# If running in Google Colab, you can optionally mount Drive,\n","# but the notebook does not depend on it.\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","except ImportError:\n","    IN_COLAB = False\n","\n","# Base directory for Sentinel-2 .jp2 files.\n","# In Colab: uses Google Drive (/content/drive/MyDrive/Sentinel) by default.\n","# For local testing: create ./data/Sentinel and place your .jp2 files there.\n","BASE_DIR = \"/content/drive/MyDrive/Sentinel\"\n","\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    # Option 1: use data stored in the project directory on Colab\n","    # BASE_DIR = \"/content/data/Sentinel\"\n","    # Option 2: uncomment the line below if the user wants to use their own Drive folder:\n","    # BASE_DIR = \"/content/drive/MyDrive/Sentinel\"\n","\n","print(\"Using data directory:\", BASE_DIR)\n","if not os.path.isdir(BASE_DIR):\n","    print(\"Directory does not exist. Please create it and put Sentinel-2 .jp2 files there.\")\n"],"metadata":{"id":"guBwProaWAED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sentinel-2 image matching loop: read from Google Drive folder and process pairs\n","\n","# Mount Google Drive\n","#drive.mount('/content/drive')\n","\n","# Base directory with Sentinel-2 .jp2 files\n","#BASE_DIR = \"/content/drive/MyDrive/Sentinel\"\n","\n","def list_jp2_files(base_dir):\n","    \"\"\"Return sorted list of .jp2 files in the given directory.\"\"\"\n","    files = [f for f in os.listdir(base_dir) if f.lower().endswith(\".jp2\")]\n","    files.sort()\n","    return files\n","\n","while True:\n","    jp2_files = list_jp2_files(BASE_DIR)\n","    if len(jp2_files) < 2:\n","        print(\"Not enough .jp2 files in directory:\", BASE_DIR)\n","        break\n","\n","    print(\"\\nAvailable .jp2 files:\")\n","    for idx, name in enumerate(jp2_files):\n","        print(f\"{idx}: {name}\")\n","\n","    # Select two files by index\n","    try:\n","        idx1 = int(input(\"\\nEnter index of FIRST image: \"))\n","        idx2 = int(input(\"Enter index of SECOND image: \"))\n","    except ValueError:\n","        print(\"Invalid input. Please enter integer indices.\")\n","        continue\n","\n","    if not (0 <= idx1 < len(jp2_files)) or not (0 <= idx2 < len(jp2_files)):\n","        print(\"Index out of range. Try again.\")\n","        continue\n","    if idx1 == idx2:\n","        print(\"Indices must be different. Try again.\")\n","        continue\n","\n","    img1_path = os.path.join(BASE_DIR, jp2_files[idx1])\n","    img2_path = os.path.join(BASE_DIR, jp2_files[idx2])\n","\n","    print(\"\\nSelected images:\")\n","    print(\"Image 1:\", jp2_files[idx1])\n","    print(\"Image 2:\", jp2_files[idx2])\n","\n","    # Read both images\n","    img1 = read_image_any(img1_path)\n","    img2 = read_image_any(img2_path)\n","\n","    # Optional: downscale to reduce memory load\n","    scale_factor = 0.3\n","    if scale_factor != 1.0:\n","        img1_small = cv2.resize(img1, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n","        img2_small = cv2.resize(img2, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n","    else:\n","        img1_small, img2_small = img1, img2\n","\n","    # Quick visualization\n","    plt.figure(figsize=(10, 4))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(img1_small)\n","    plt.title(f\"Image 1 ({jp2_files[idx1]})\")\n","    plt.axis(\"off\")\n","\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(img2_small)\n","    plt.title(f\"Image 2 ({jp2_files[idx2]})\")\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","    # Run both algorithms\n","    print(\"\\nRunning ORB + RANSAC...\")\n","    run_orb_matching(img1_small, img2_small)\n","\n","    print(\"\\nRunning LoFTR matching...\")\n","    run_loftr_matching(img1_small, img2_small)\n","\n","    # Ask if user wants to continue\n","    ans = input(\"\\nDo you want to process another pair of images? (y/n): \").strip().lower()\n","    if ans != \"y\":\n","        print(\"Stopping interactive loop.\")\n","        break\n"],"metadata":{"id":"g6FL6xd-co_T"},"execution_count":null,"outputs":[]}]}